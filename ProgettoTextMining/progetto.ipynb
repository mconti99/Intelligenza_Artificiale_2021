{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RappresentazioneInformazioni.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYy5snWz0CcP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So-__C9I0s7E"
      },
      "source": [
        "# Estrazione di informazioni testuali da pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69CkffLyM16"
      },
      "source": [
        "In questa sezione ci occupiamo di effettuare l'operazione di scanning dei dati che sono caricati da un pdf caricato su drive. Per caricare i dati contenuti nei pdf utilizziamo gli appositi metodi forniti dalla libreria pythin PyPDF2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it7j0-si0wc5"
      },
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfFileReader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lme-3fjGypOo"
      },
      "source": [
        "Apertura del file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeFkE3_U2ga1"
      },
      "source": [
        "pdfFileObj = open(r'/content/drive/MyDrive/ontologiaPDF.pdf', mode='rb')\n",
        "PdfLeggi = PdfFileReader(pdfFileObj, strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYVyg02Sy2IT"
      },
      "source": [
        "Convertiamo le singole pagine del pdf in stringhe python che sono poi unite nella stringa testo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3E589mx4sCU"
      },
      "source": [
        "pages_number = 5\n",
        "pagina = list()\n",
        "testo_pagina = list()\n",
        "testo = ''\n",
        "for i in range(pages_number):\n",
        "  pagina.append(PdfLeggi.getPage(i))\n",
        "  testo_pagina.append(pagina[i].extractText())\n",
        "  testo = testo + testo_pagina[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91F7J3HC6EGE"
      },
      "source": [
        "# Estrazione di informazioni da testo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LVEyPEpzN3J"
      },
      "source": [
        "Ci occupiamo ora delle elaborazione del testo precedentemente caricato utilizzando la libreria nltk, una libreria etremamente potente che ci fornisce tutti gli strumenti fondamentali per il NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx21o-DY6NMz"
      },
      "source": [
        "## Implementazione di una Pipeline NLP in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWkZHkrOzYKA"
      },
      "source": [
        "Come fase iniziale di pretrattamento del testo andiamo ad effettuare delle operazioni di parsing: in particolarmodo eseguiamo un'operazione di tokenizzazione scomponendo il testo in sigole parole o nelle singole frasi che lo compongono."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndv4BNZ16eKI"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all', quiet=True)\n",
        "token = nltk.word_tokenize(testo)\n",
        "frasi = nltk.sent_tokenize(testo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFjEMm3j0D8i"
      },
      "source": [
        "Segue poi la fase di annotazione sintattica: sfruttando il comando pos_tag della libreria nltk andiamo ad effettuare il tagging grammaticale, associando ad ogni token prodotto nella fase precedente la sua categoria grammaticale:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbJbcAkU_GVN"
      },
      "source": [
        "POS_tag = nltk.pos_tag(token)\n",
        "print(POS_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0KN1HJ0ofO"
      },
      "source": [
        "Ci occupiamo a questo punto della stemmatizzazione, andando ad associare ad ogni parola del testo il suo stem, utilizzando lo SNowballStemmer fornito da nltk:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2htTyeVCs-T"
      },
      "source": [
        "from nltk.stem import SnowballStemmer \n",
        "radice = SnowballStemmer('english')\n",
        "# radice.stem(token[10]) \n",
        "for word in token:\n",
        "  stem = radice.stem(word)\n",
        "  print('parola {}, stem: {}'.format(word, stem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u21eBXl1Dux"
      },
      "source": [
        "Vogliamo procedere ora alla fase di lemmatizzazione: ad ogni parola del testo vogliamo associare la sua forma di base. Per far ciò utilizzaimo il WordNetLemmatizer che sfrutta un database in internet. Prima però dobbiamo definire una funzione che mappi i PosTag estratti in precedenza nel corretto formato previsto dal lemmatizzatore:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE0jwqHyDZGj"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import *\n",
        "\n",
        "lemmatizzatore = WordNetLemmatizer()\n",
        "\n",
        "def map_postag_to_lemtag(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif treebank_tag.startswith('S'):\n",
        "        return wordnet.ADJ_SAT\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2S83sS1ajt"
      },
      "source": [
        "Eseguiamo ora un loop che, ad ogni parola, e corrispondente POS_tag, associ il corrispondente lemma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzMAAors487T"
      },
      "source": [
        "for i,j in POS_tag:\n",
        "  pos = map_postag_to_lemtag(j)\n",
        "  lemma = lemmatizzatore.lemmatize(i.lower(),pos.lower())\n",
        "  print('parola: {}, lemma: {}'.format(i, lemma))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFflUcfHO5E7"
      },
      "source": [
        "Ci occupiamo a questo punto del chunking: cerchiamo all'interno del testo diversi sintagmi nominali definiti da delle regole \"grammar\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d37CFfi5PIpi"
      },
      "source": [
        "Cerchiamo sintagmi composti da articolo, nome e aggettivo:\n",
        "\n",
        "* le regole sono composte con il seguente formato: grammatica=\"etichetta:{\\<POS1>\\<POS2>\\<POS3>}\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fK9DCT_dxW9"
      },
      "source": [
        "grammar = \"SintagmaNominale:{<DT><JJ><NN>}\"\n",
        "cp = nltk.RegexpParser(grammar) \n",
        "sintagmiNominali = cp.parse(POS_tag)\n",
        "print(sintagmiNominali)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ER_48ePQOi"
      },
      "source": [
        "Cerchiamo ora sintagmi più complessi definendo una regola grammaticale condizionale:\n",
        "\n",
        "\n",
        "\n",
        "*   ? se inserito permette di considerare o meno la presenza del tipo di token ricercato\n",
        "*   \\* può significare 0 volte a tante volte\n",
        "*  \\+ significa almeno una volta oppure tante volte\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuvXKgLUkuas"
      },
      "source": [
        "grammar2 = \"SintagmaNominale2:{<DT>?<JJ>*<NN>+}\"\n",
        "cp2 = nltk.RegexpParser(grammar2)\n",
        "sintagmiNominali2 = cp2.parse(POS_tag)\n",
        "print(sintagmiNominali2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOuxU2oIPfR-"
      },
      "source": [
        "Cerchiamo sintagmi composti da nome + verbo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZYG0tn1yk"
      },
      "source": [
        "grammar3 = \"SintagmaNominale3:{<NN><VB.*>}\"\n",
        "cp3 = nltk.RegexpParser(grammar3)\n",
        "sintagmiNominali3 = cp3.parse(POS_tag)\n",
        "print(sintagmiNominali3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tyct8SFPy7T"
      },
      "source": [
        "Cerchiamo in fine sintagmi di diversa natura con una regola or:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr253DaVpAW0"
      },
      "source": [
        "grammar4 = r\"\"\"\n",
        "Sintagma3elementiArtAggNom:{<DT>?<JJ>*<NN>+}\n",
        "Sintagma2elementiNomeVerbo:{<NN><VB>} \n",
        "Sintagma1elementoVerbo:{<VB.*>*}\n",
        "\"\"\"\n",
        "\n",
        "cp4 = nltk.RegexpParser(grammar4)\n",
        "sintagmiNominali4 = cp4.parse(POS_tag)\n",
        "print(sintagmiNominali4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmkzbIkLx_90"
      },
      "source": [
        "# Ricerca sinomini, termini correlati"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hp5QW2zP5oU"
      },
      "source": [
        "Cerchiamo dal database wordnet i sinonimi di una parola che sostituiti nel testo non ne stravolgono il significato:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r8LJ4zHyF7M"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "output = wn.synsets('home','a')\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrWHHXnAQGeT"
      },
      "source": [
        "Cerchiamo in fine la descrizione ed una frase di esempio contenete la parola di interesse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EXNHf4r1FUM"
      },
      "source": [
        "parola_che_mi_interessa = wn.synset('home.s.03') \n",
        "print(parola_che_mi_interessa.definition()) \n",
        "print(parola_che_mi_interessa.examples())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}